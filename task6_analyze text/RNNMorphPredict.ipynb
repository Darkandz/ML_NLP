{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwilKfnz78uR",
        "outputId": "9daff8ab-0536-4ea1-f076-fe8ece5bc435"
      },
      "source": [
        "!pip install rnnmorph"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rnnmorph\n",
            "  Downloading rnnmorph-0.4.1.tar.gz (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.8/dist-packages (from rnnmorph) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.8/dist-packages (from rnnmorph) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from rnnmorph) (1.0.2)\n",
            "Requirement already satisfied: keras>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from rnnmorph) (2.11.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from rnnmorph) (3.1.0)\n",
            "Collecting pymorphy2>=0.8\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting russian-tagsets==0.6\n",
            "  Downloading russian-tagsets-0.6.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.8/dist-packages (from rnnmorph) (4.64.1)\n",
            "Collecting jsonpickle>=0.9.4\n",
            "  Downloading jsonpickle-3.0.1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.5 in /usr/local/lib/python3.8/dist-packages (from rnnmorph) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2.5->rnnmorph) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2.5->rnnmorph) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2.5->rnnmorph) (7.1.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18.1->rnnmorph) (3.1.0)\n",
            "Building wheels for collected packages: rnnmorph, russian-tagsets, docopt\n",
            "  Building wheel for rnnmorph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rnnmorph: filename=rnnmorph-0.4.1-py3-none-any.whl size=19746378 sha256=e15d49b909f182e245e4a36bb42c3cf5633984e8c74ea1aec70437842a1d0505\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/47/48/13dc27987d34abf46ca5fb2877fb993edd01cd95153863f84d\n",
            "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for russian-tagsets: filename=russian_tagsets-0.6-py3-none-any.whl size=24637 sha256=8312d923892bc3b4a81041b422871916156d1f548511d35e58233ef36e087e30\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/2e/54/71c28ef06e79d9bdd7843ad80473900615056abb3261544039\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=6c07ccf5bd85dc028f4248ec083f0808bccc835c5b8354c4c7023f207b8feb7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built rnnmorph russian-tagsets docopt\n",
            "Installing collected packages: russian-tagsets, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, jsonpickle, rnnmorph\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 jsonpickle-3.0.1 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 rnnmorph-0.4.1 russian-tagsets-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE0iZ14_7_S5",
        "outputId": "5c1df75c-d702-4808-933e-da2280e7e754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "predictor = RNNMorphPredictor(language=\"ru\")\n",
        "forms = predictor.predict([\"мама\", \"мыла\", \"раму\"])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 5s 5s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = \"http://az.lib.ru/l/leskow_n_s/text_0246.shtml\" # Ссылка на файл из Задания\n",
        "more_words_count = 100\n",
        "tokens_more_count = 50"
      ],
      "metadata": {
        "id": "X5PTgAmSIK9o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_433i2T8GiV",
        "outputId": "deb249a4-01af-497b-9f2c-7ec3987de903"
      },
      "source": [
        "import nltk\n",
        "import warnings\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from nltk import FreqDist\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "predictor = RNNMorphPredictor(language=\"ru\")\n",
        "opener = urllib.request.URLopener({})\n",
        "resource = opener.open(DATA_URL)\n",
        "raw_text = resource.read().decode(resource.headers.get_content_charset())\n",
        "\n",
        "soup = BeautifulSoup(raw_text, features=\"html.parser\")\n",
        "\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "\n",
        "cleaned_text = soup.get_text()\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(cleaned_text)]\n",
        "predictions = [[pred.normal_form for pred in sent if pred.normal_form.isalpha()] for sent in tqdm(predictor.predict_sentences(sentences=tokenized_sentences), \"sentences\") ] # ИСПРАВИЛ НА ТОЛЬКО БУКВЫ\n",
        "non_uniq_tokens = [word for sentence in predictions for word in sentence]\n",
        "\n",
        "answers = {}\n",
        "for i in non_uniq_tokens:\n",
        "  if i in answers:\n",
        "    answers[i] += 1\n",
        "  else:\n",
        "    answers[i] = 1\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"russian\"))\n",
        "\n",
        "t = 1\n",
        "counter = more_words_count + 1\n",
        "while counter > more_words_count:\n",
        "  counter = 0\n",
        "  n_answers = []\n",
        "  for i in answers:\n",
        "    if (answers[i] > t):\n",
        "      n_answers.append(i)\n",
        "      counter += 1\n",
        "  t += 1\n",
        "\n",
        "counter = 0\n",
        "for i in n_answers:\n",
        "  if i in stopwords.words(\"russian\"):\n",
        "    counter += 1\n",
        "\n",
        "counter1 = 0\n",
        "for i in answers:\n",
        "  if (answers[i] > tokens_more_count):\n",
        "    counter1 += 1\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Введите количество предложений\")\n",
        "print(\"Ответ:\", len(predictions))\n",
        "\n",
        "print(\"Введите количество токенов, состоящих только из букв\")\n",
        "print(\"Ответ:\", len(non_uniq_tokens))\n",
        "\n",
        "print(f\"Какую долю среди {more_words_count} самых частотных токенов в произведении занимают слова, не входящие в стоп-лист?\")\n",
        "print(\"Ответ:\", (more_words_count - counter) / more_words_count)\n",
        "\n",
        "print(f\"Сколько токенов встречается в тексте строго больше {tokens_more_count} раз?\")\n",
        "print(\"Ответ:\", counter1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 6s 563ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sentences: 100%|██████████| 466/466 [00:00<00:00, 62104.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Введите количество предложений\n",
            "Ответ: 466\n",
            "Введите количество токенов, состоящих только из букв\n",
            "Ответ: 9053\n",
            "Какую долю среди 100 самых частотных токенов в произведении занимают слова, не входящие в стоп-лист?\n",
            "Ответ: 0.46\n",
            "Сколько токенов встречается в тексте строго больше 50 раз?\n",
            "Ответ: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6KEkleH8Hty",
        "outputId": "02fd229d-cd8f-4ee9-943e-371140ac5e74"
      },
      "source": [
        "print(forms[0].tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Case=Nom|Gender=Fem|Number=Sing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVwzQ6rQ8IdU",
        "outputId": "ee4e23b7-d488-4ab1-8066-cdb0bc1147c7"
      },
      "source": [
        "print(forms[0].normal_form)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мама\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDy_eYbt8JJR",
        "outputId": "8308b0ea-43e4-4977-8b05-a26a9b598084"
      },
      "source": [
        "print(forms[0].vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1]\n"
          ]
        }
      ]
    }
  ]
}